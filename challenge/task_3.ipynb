{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7cfc4fb",
   "metadata": {},
   "source": [
    "# Fullstack GPT: #5.0 ~ #5.8\n",
    "\n",
    "- Tasks:\n",
    "    - 앞서 배운 메모리 클래스 중 하나를 사용하는 메모리로 LCEL 체인을 구현합니다.\n",
    "        - [x] [ConversationBufferMemory](https://python.langchain.com/v0.1/docs/modules/memory/types/buffer/) 등 강의에서 배운 메모리 중 하나를 사용하여 이전 대화 기록을 기억하고 기록을 이용한 답변을 제공할 수 있도록 합니다.\n",
    "        - [x] 채팅 형식의 메모리 기록을 프롬프트에 추가하고 싶을 때는 [MessagesPlaceholder](https://python.langchain.com/v0.1/docs/modules/memory/adding_memory/#adding-memory-to-a-chat-model-based-llmchain)를 이용하세요.\n",
    "        - [x] RunnablePassthrough를 활용하면 LCEL 체인을 구현할 때 메모리 적용을 쉽게 할 수 있습니다. RunnablePassthrough는 메모리를 포함한 데이터를 체인의 각 단계에 전달하는 역할을 합니다. (강의 #5.7 1:04~ 참고)\n",
    "    - [x] The chain should take the title of a movie and reply with three emojis that represent the movie. (i.e \"Top Gun\" -> \"🛩️👨‍✈️🔥\". \"The Godfather\" -> \"👨‍👨‍👦🔫🍝 \").\n",
    "    - [x] 항상 세 개의 이모티콘으로 답장하도록 [FewShotPromptTemplate](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/few_shot_examples/) 또는 [FewShotChatMessagePromptTemplate](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/few_shot_examples_chat/)을 사용하여 체인에 예시를 제공하세요.\n",
    "    - [x] To check that the memory is working ask the chain about two movies and then in another cell ask the chain to tell you what is the movie you asked about first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141f8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"question\": \"Top Gun\", \"answer\": \"🛩️👨‍✈️🔥\"},\n",
    "    {\"question\": \"The Godfather\", \"answer\": \"👨‍👨‍👦🔫🍝\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d895ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💌🌧️💖🎤👹⚔️"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})['history']\n",
    "\n",
    "def invoke_chain(movie_title):\n",
    "    result = chain.invoke({\"question\": movie_title})\n",
    "    memory.save_context({\"input\": movie_title}, {\"output\": result.content})\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{question}\"),\n",
    "    (\"ai\", \"{answer}\"),\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a movie geek. Take the title of a movie and reply with three emojis that represent the movie.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "invoke_chain(\"Love Letter\")\n",
    "invoke_chain(\"K-pop Demon Hunters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c8736d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💌🌧️💖"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What was the first movie i've asked?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5d420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 ✍️ 🇰🇷❌🚪🔒"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "def invoke_chain(movie_title):\n",
    "    result = chain.invoke({\"question\": movie_title})\n",
    "    memory.save_context({\"input\":movie_title}, {\"output\":result.content})\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    Human: {question},\n",
    "    AI: {answer}\n",
    "\"\"\")\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"{question}\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "    Take the title of a movie and reply with three emojis that represent the movie.\n",
    "                                      \n",
    "    {history}\n",
    "    {few_shot_prompt}\n",
    "                                      \n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "    history = lambda x: memory.load_memory_variables({})['history'],\n",
    "    few_shot_prompt = lambda x: few_shot_prompt,\n",
    "    )\n",
    "| prompt\n",
    "| llm\n",
    ")\n",
    "\n",
    "invoke_chain(\"DongJu\")\n",
    "invoke_chain(\"No Other Choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb0d5eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DongJu"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='DongJu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"what is the movie i asked about first looking at history?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
