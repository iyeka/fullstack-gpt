{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300d61ee",
   "metadata": {},
   "source": [
    "# FullStack GPT: #6.0 ~ #6.10\n",
    "\n",
    "- [x] Implement a complete RAG pipeline with a Stuff Documents chain.\n",
    "- [x] You must implement the chain manually.\n",
    "- [x] Give a ConversationBufferMemory to the chain.\n",
    "- [x] Use this document to perform RAG: https://gist.github.com/serranoarevalo/5acf755c2b8d83f1707ef266b82ea223\n",
    "- [x] Ask the following questions to the chain:\n",
    "    - Is Aaronson guilty?\n",
    "    - What message did he write in the table?\n",
    "    - Who is Julia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca13712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 538, which is longer than the specified 500\n",
      "Created a chunk of size 511, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question)\n",
    "    memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "\n",
    "def get_history(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "loader = TextLoader(\"../files/document.txt\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "cache_dir = LocalFileStore(\"../.cache/\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "     You are a helpful assistant. Answer the question based solely on the given context and chat history. If you don't know the answer, don't make it up.\n",
    "     -----\n",
    "     {context}\n",
    "    \"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = {\n",
    "    \"context\": retriever, \n",
    "    \"history\": RunnableLambda(get_history), \n",
    "    \"question\": RunnablePassthrough()\n",
    "    } | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e686c960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Aaronson guilty?\n",
      "In the context provided, Winston reflects that Aaronson, along with Jones and Rutherford, is guilty of the crimes they were charged with, but he also acknowledges that he has never seen the photograph that disproved their guilt and concludes that it had never existed. This suggests that the Party's claims about Aaronson's guilt are questionable and likely a manipulation of truth. Therefore, while the Party asserts that Aaronson is guilty, the reality of his guilt is uncertain.\n",
      "\n",
      "What message did he write in the table?\n",
      "Winston wrote \"FREEDOM IS SLAVERY\" and beneath it \"TWO AND TWO MAKE FIVE\" on the table. Later, he also wrote \"GOD IS POWER.\"\n",
      "\n",
      "Who is Julia?\n",
      "Julia is a character in the story who is Winston's love interest. She works for the Party but secretly opposes its oppressive regime. Julia engages in a romantic relationship with Winston, and their connection symbolizes rebellion against the Party's control over personal relationships and individual freedoms. Throughout the narrative, Julia's character embodies themes of love, desire, and resistance.\n",
      "\n",
      "<function get_history at 0x108313f60>\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "questions = [\"Is Aaronson guilty?\", \"What message did he write in the table?\", \"Who is Julia?\"]\n",
    "\n",
    "for question in questions:\n",
    "    print(question)\n",
    "    chain.invoke(question)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
