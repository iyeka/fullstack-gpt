{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2998ee",
   "metadata": {},
   "source": [
    "# Fullstack GPT: #6.0 ~ #6.10\n",
    "\n",
    "- Tasks:\n",
    "- [x] Stuff Documents 체인을 사용하여 완전한 RAG 파이프라인을 구현하세요.\n",
    "- [x] 체인을 수동으로 구현해야 합니다.\n",
    "- [x] 체인에 ConversationBufferMemory를 부여합니다.\n",
    "- [x] 이 문서를 사용하여 RAG를 수행하세요: https://gist.github.com/serranoarevalo/5acf755c2b8d83f1707ef266b82ea223\n",
    "- [x] 체인에 다음 질문을 합니다:\n",
    "    - [x] Is Aaronson guilty?\n",
    "    - [x] What message did he write in the table?\n",
    "    - [x] Who is Julia?\n",
    "- 다음과 같은 절차대로 구현하면 챌린지를 해결할 수 있습니다.\n",
    "    - [x] (1) 문서 로드하기 : [TextLoader](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/) 등 을 사용해서 파일에서 텍스트를 읽어옵니다.\n",
    "    - [x] (2) 문서 쪼개기 : [CharacterTextSplitter](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/character_text_splitter/) 등 을 사용해서 문서를 작은 문서 조각들로 나눕니다.\n",
    "    - [x] (3) 임베딩 생성 및 캐시 : OpenAIEmbeddings, [CacheBackedEmbeddings](https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/caching_embeddings/) 등 을 사용해 문서 조각들을 임베딩하고 임베딩을 저장합니다.\n",
    "    - [x] (4) 벡터 스토어 생성 : [FAISS](https://python.langchain.com/v0.1/docs/integrations/vectorstores/faiss/) 등 을 사용해서 임베딩된 문서들을 저장하고 검색할 수 있는 데이터베이스를 만듭니다.\n",
    "    - [x] (5) 대화 메모리와 질문 처리 : ConversationBufferMemory를 사용해 대화 기록을 관리합니다.\n",
    "    - [x] (6) 체인 연결 : 앞에서 구현한 컴포넌트들을 적절하게 체인으로 연결합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f41a5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Is Aaronson guilty?'),\n",
       " AIMessage(content=\"According to the context provided, Winston believes that Aaronson, along with Jones and Rutherford, is guilty of the crimes they were charged with. However, he also acknowledges that he has never seen the photograph that disproved their guilt and that it had never existed, implying that their guilt is a fabrication of the Party. Therefore, while the Party claims Aaronson is guilty, Winston's realization suggests that the truth is more complex and likely indicates that Aaronson is not actually guilty.\"),\n",
       " HumanMessage(content='What message did he write in the table?'),\n",
       " AIMessage(content='Winston traced \"2+2=5\" in the dust on the table.'),\n",
       " HumanMessage(content='Who is Julia?'),\n",
       " AIMessage(content=\"Julia is a character in the story who is Winston's love interest. She is a fellow Party member who initially appears to conform to the Party's rules but secretly rebels against its oppressive regime. Julia and Winston share a romantic relationship that symbolizes their resistance to the Party's control over their personal lives and emotions. Throughout the narrative, she represents both a source of personal happiness for Winston and a connection to the idea of rebellion against the totalitarian state.\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def embedder():\n",
    "    FILE_PATH = \"../files/document.txt\"\n",
    "\n",
    "    loader = TextLoader(FILE_PATH)\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    docs = loader.load_and_split(splitter)\n",
    "\n",
    "    cache_dir = LocalFileStore(\"../files/.cache/\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embbedings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embbedings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "def invoke_chain(query):\n",
    "    response = chain.invoke(query)\n",
    "    memory.save_context({\"input\":query}, {\"output\": response.content})\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "retriever = embedder()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    Answer human question according to following context and conversation history. If you don't know don't make it up and just say so.\\n\\n\n",
    "    {context}\n",
    "    \"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = {\"context\":retriever, \"history\":RunnableLambda(load_memory), \"question\":RunnablePassthrough()} | prompt | llm\n",
    "invoke_chain(\"Is Aaronson guilty?\")\n",
    "invoke_chain(\"What message did he write in the table?\")\n",
    "invoke_chain(\"Who is Julia?\")\n",
    "\n",
    "load_memory(\"input\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
