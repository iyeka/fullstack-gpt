{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac96b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 LLMs and Chat Models\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "llm.predict(\"How many planets are there?\")\n",
    "chat.predict(\"How many planets are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99087e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Predict Messages with Chat Model\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are Madalena Bay. Reply as if you were her.\"),\n",
    "    AIMessage(content=\"It's me, Imaginal Disk\"),\n",
    "    HumanMessage(content=\"What songs you are influenced by?\")\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 PromptTemplate without from_template\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "template.format(country=\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446a713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Customize prompt with Prompt Template\n",
    "\n",
    "from langchain.chat_models import  ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"What songs did Magdalena Bay listen when making {track}?\"\n",
    ")\n",
    "prompt = template.format(track=\"Imaginal Disk\")\n",
    "\n",
    "chat.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Customize prompt with Chat Prompt Template\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "    (\"ai\", \"Ciao, mi chiamo {name}!\"),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"What is the distance between {country_a} and {country_b}. Also, what is your name?\",\n",
    "    ),\n",
    "])\n",
    "prompt = template.format_messages(\n",
    "    language=\"Greek\",\n",
    "    name=\"Socrates\",\n",
    "    country_a=\"Mexico\",\n",
    "    country_b=\"Thailand\",\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae118e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Concept of OutputParser\n",
    "\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(',')\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "p.parse(\"It's ,you, the,  purest,you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Apply OutputParser\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(',')\n",
    "        return list(map(str.strip, items))\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "    \"\"\"\n",
    "     You are a list generating machine. \n",
    "     Answer question with these conditions.\n",
    "     - COMMA SEPERATED LIST\n",
    "     - List have maximum {max_items} items.\n",
    "     - All characters in lowercase\n",
    "    \"\"\"),\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "prompt = template.format_messages(\n",
    "    max_items=5,\n",
    "    question=\"What are the pokemons?\"\n",
    ")\n",
    "\n",
    "result = chat.predict_messages(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 OutputParser and LCEL\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "            \"system\",\n",
    "            \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase.Do NOT reply with anything else.\",\n",
    "    ),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(',')\n",
    "        return list(map(str.strip, items))\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\"max_items\":5, \"question\":\"What are the pokemons?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091261e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Chaining Chains\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # listening to events on LLM\n",
    ")\n",
    "original_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a pro baker. You create a easy to follow recipe that anyone can succeed with.\"),\n",
    "    (\"human\", \"I want to bake {bread_type}.\")\n",
    "])\n",
    "original_chain = original_prompt | chat\n",
    "\n",
    "modify_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are a Korean baker. You change the original baking recipe to Koreans love. You add and adjust some ingredients. If you don't know how to convert, just say so.\"),\n",
    "    (\"human\", \"{recipe}\")\n",
    "])\n",
    "modify_chain = modify_prompt | chat\n",
    "\n",
    "final_chain = {\"recipe\": original_chain} | modify_chain\n",
    "# modify_chain.invoke({\"recipe\": original_chain_result})\n",
    "\n",
    "final_chain.invoke({\"bread_type\": \"Country Bread\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 FewShotPromptTemplate: Give examples to the model how to answer.\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# -- 1. Get examples from database\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# -- 2. Create prompt to format examples\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "# example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "# -- 3. Give examples to the FewShotPromptTemplate\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\", # same format as the question of examples\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"country\":\"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 FewShotChatMessagePromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "    (\"ai\", \"{answer}\"),\n",
    "])\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert. You give short answers.\"),\n",
    "    example_prompt,\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "chain.invoke({\"country\":\"Turkey\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 LengthBasedExampleSelector: 비용이나 모델 제약 때문에 모델에 보낼 examples를 select 해야할 때\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=80,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Making your own example selector: Random Example Selector\n",
    "'''\n",
    "ex)\n",
    "- if user is logged-in or not\n",
    "- language of the user \n",
    "'''\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples: list[dict[str, str]]):\n",
    "        self.examples = examples\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "    \n",
    "    def add_example(self, example): # add_example to already existing examples list\n",
    "        self.examples.append(example)\n",
    "    \n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "example_prompt = PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "example_selector = RandomExampleSelector(examples=examples)\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "prompt.format(country=\"Brazil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc105e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Serialization: Load PromptTemplates from disk\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "json_prompt = load_prompt(\"./prompt.json\")\n",
    "yaml_prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "json_prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425fcd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Composition: 여러 prompts 합치기\n",
    "\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "\n",
    "    {example}\n",
    "\n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "chain = full_prompt | chat\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a4394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Caching: Save responses from LM\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache # memory에 caching하기 때문에 notebook을 재시작하면 refreshed.\n",
    "from langchain.cache import SQLiteCache # Save in database\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "chat.predict(\"How to make Italian Pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 SetDebug: 무슨 일을 하고 있는지 로그 기록을 보여준다.\n",
    "\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "set_debug=True\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "chat.predict(\"How to make Italian Pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef377f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 지출 비용 알리미\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju?\")\n",
    "    b = chat.predict(\"What is the recipe for sourdough?\")\n",
    "    print(usage)\n",
    "    print(usage.total_cost)\n",
    "    print(usage.total_tokens)\n",
    "    print(usage.prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b214abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Serialization: Save models\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "chat = OpenAI(\n",
    "    temperature=0.1,\n",
    "    model=\"gpt-5-nano\",\n",
    "    max_tokens=450,\n",
    ")\n",
    "\n",
    "chat.save(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Serialization: Load models\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "chat = load_llm(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4546acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 ConversationBufferMemory\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory() # returns text\n",
    "memory = ConversationBufferMemory(return_messages=True) # returns chat_message\n",
    "\n",
    "memory.save_context({\"input: Hi!\", \"output: How are you\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 ConversationBufferWindowMemory\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    "    k=1,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input, \"output\":output})\n",
    "\n",
    "memory.save_context(1,1) # gone\n",
    "memory.save_context(2,2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 ConversationSummaryMemory\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input, \"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"i'm so sleepy\", \"you better get sleep\")\n",
    "add_message(\"Let's go to bed\", \"It's too early\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962a9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 ConversationSummaryBufferMemory\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=10, # 메시지를 요약하기까지 max_token\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input, \"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"i'm so sleepy\", \"you better get sleep\")\n",
    "add_message(\"Let's go to bed\", \"It's too early\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f68a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 ConversationKGMemory\n",
    "\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input, \"output\":output})\n",
    "\n",
    "add_message(\"Nico likes Kimchi\", \"you better get sleep\")\n",
    "\n",
    "memory.load_memory_variables({\"input\": \"What does Nico likes?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Text Memory on LLMChain: off-the-shelf(general purpose) chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1),\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful AI talking to human. \n",
    "{chat_history}\n",
    "Human: {question}\n",
    "You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")\n",
    "chain.predict(question=\"I live in Seoul\")\n",
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 Chat Based Memory on LLMChain\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"haman\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b501edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 LCEL Based Memory\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\n",
    "    \"history\": memory.load_memory_variables({})[\"history\"],\n",
    "    \"question\": \"My name is Nico\",\n",
    "})\n",
    "# OR\n",
    "def load_memory(_): # ignore the input\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "chain.invoke({\"question\":\"My name is Nico\"})\n",
    "\n",
    "# save memory\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\":question})\n",
    "    memory.save_context({\"input\":question}, {\"output\":result.content})\n",
    "    print(result)\n",
    "invoke_chain(\"My name is nico\")\n",
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.1 The more smaller the chunks are, the better.\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "txt_loader = TextLoader(\"./.cache/files/document.txt\")\n",
    "pdf_loader = PyPDFLoader(\"./.cache/files/document.pdf\")\n",
    "anything_loader = UnstructuredFileLoader(\"./.cache/files/document.txt\")\n",
    "docs = anything_loader.load()\n",
    "\n",
    "chunk_size_splitter = RecursiveCharacterTextSplitter()\n",
    "chunk_size_splitter.split_documents(docs)\n",
    "# OR\n",
    "anything_loader.load_and_split(text_splitter=chunk_size_splitter)\n",
    "\n",
    "chunk_size_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200, # max characters\n",
    "    chunk_overlap=50, # This is for not to cutting paragraphs in the middle\n",
    ")\n",
    "\n",
    "separator_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\", # split by paragraph\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Tiktoken: Count Tokens by statistical relationships\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "separator_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len, # default python standard library function\n",
    ")\n",
    "\n",
    "separator_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Embedding Models\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "vector = embedder.embed_query(\"Hi\")\n",
    "vector = embedder.embed_documents([\"Hi\", \"How\", \"Are\", \"You\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e39afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Embed: 쪼갠 문서마다 vector를 만든다.\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "loader = TextLoader(\"./cache/files/document.txt\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "\n",
    "# 1 Check if embeddings exits in cache\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cached_embbedings = CacheBackedEmbeddings(embeddings, cache_dir)\n",
    "\n",
    "# 2 If not, use OpenAIEmbeddings and cache those.\n",
    "vectorstore = Chroma.from_documents(docs, cached_embbedings)\n",
    "\n",
    "results = vectorstore.similarity_search(\"Where does Winston lives?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6 Stuff Chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "loader = TextLoader(\"././cace/files/document.txt\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embbedings = CacheBackedEmbeddings(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embbedings)\n",
    "\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(), # retrieve from where\n",
    ")\n",
    "chain.invoke(\"Where does Winston Love?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.8 Stuff LCEL Chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "loader = TextLoader(\"./.cache/files/document.txt\")\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = loader.load_and_split(splitter)\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/embeddings/\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embbedings = CacheBackedEmbeddings(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embbedings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = {\"context\": retriever, \"question\":RunnablePassthrough()} | prompt | llm\n",
    "chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.9 Map Reduce LCEL Chain\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def embedder():\n",
    "    FILE_PATH = \"./files/example_document.txt\"\n",
    "\n",
    "    loader = UnstructuredFileLoader(FILE_PATH)\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    docs = loader.load_and_split(splitter)\n",
    "\n",
    "    cache_dir = LocalFileStore(\"./files/.cache/\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embbedings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embbedings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "retriever = embedder()\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "        -------\n",
    "        {context}\n",
    "        \"\"\",\n",
    "    ),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke({\n",
    "            \"context\": document.page_content,\n",
    "            \"question\": question,\n",
    "        }).content\n",
    "        for document in documents\n",
    "    )\n",
    "map_chain = {\"documents\": retriever, \"question\": RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        Given the following extracted parts of a long document and a question, create a final answer. \n",
    "        If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "        ------\n",
    "        {context}\n",
    "        \"\"\",\n",
    "    ),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
