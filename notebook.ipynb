{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 Predict Text with LLMs VS Chat Models\n",
    "\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "a = llm.predict(\"How many planets are there?\")\n",
    "b = chat.predict(\"How many planets are there?\")\n",
    "\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Predict Messages with Chat Model\n",
    "\n",
    "# import message constructors\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a geography expert. You only reply in Italian.\"),\n",
    "    AIMessage(content=\"Ciao, mi chiamo Paolo.\"),\n",
    "    HumanMessage(content=\"What is the distance btw Mexico and Thailand? Also, what is your name?\"),\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 PromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "template = PromptTemplate.from_template(\"What is the distance btw {country_a} and {country_b}\")\n",
    "prompt = template.format(country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "\n",
    "chat.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cdbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 PromptTemplate without from_template method\n",
    "\n",
    "t = PromptTemplate(\n",
    "    template=\"What is the capital of {country}\",\n",
    "    input_variables=[\"country\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 ChatPromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geographic expert. You only reply in {language}.\"),\n",
    "    (\"ai\", \"Ciao, Mi chiamo {name}!\"),\n",
    "    (\"human\", \"What is the distance btw {country_a} and {country_b}\"),\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    language=\"Greek\",\n",
    "    name=\"Paolo\",\n",
    "    country_a=\"Mexico\",\n",
    "    country_b=\"Thailand\",\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fbb716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 OutputParser: LLM의 Text response를 list로 변환\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        list_items = text.strip().split(\",\")\n",
    "        # ['Hello','How','are ',' you']\n",
    "        return list(map(str.strip(), list_items))\n",
    "        # ['Hello','How','are','you'] map each item with method.\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "p.parse(\"Hello,How,are , you\")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a list generatin machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do NOT reply with anything else.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question=\"What are the colours?\",\n",
    ")\n",
    "\n",
    "result = chat.predict_messages(prompt)\n",
    "# AIMessage(content='red, orange ... ')\n",
    "p.parse(result.content)\n",
    "# ['red', 'orange' ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abba543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 LangChain Expression Language (LCEL)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "p = CommaOutputParser()\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a list generatin machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do NOT reply with anything else.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"What are the Pokemons?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb03acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Chaining Chains\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world-class international chef. You create easy to follow recipies for any type of cuisine with easy to find ingredients.\"),\n",
    "    (\"human\", \"I want to cook {cuisine} food.\")\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat\n",
    "\n",
    "vegeterian_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a vegetarian chef specialized on making traditional recipes vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to recipe it.\"), (\"human\", \"{recipe}\")\n",
    "])\n",
    "\n",
    "veg_chain = vegeterian_prompt | chat\n",
    "\n",
    "              # 2 Runnable Map       | # 3\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "# 1\n",
    "final_chain.invoke({\"cuisine\":\"indian\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8618d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 FewShotPromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=StreamingStdOutCallbackHandler(),\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Same as examples {variables}\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    # Same format as examples. Question of the user:\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    # validation:\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "prompt.format(country=\"Germany\")\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"country\": \"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37692743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 FewShotChatMessagePromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"What do you know about {country}\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert. You give short answers.\"),\n",
    "    example_prompt,\n",
    "    (\"human\", \"What do you know about {country}?\")\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "chain.invoke({\"country\": \"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8df718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 LengthBasedExampleSelector\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, LengthBasedExampleSelector\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "example_prompt = ChatPromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=80,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d2cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Custom Example Selector\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(examples)]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "example_selector = RandomExampleSelector(examples=examples)\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"coutry\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Serialization: Save and load prompt\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "json_prompt = load_prompt(\"./prompt.json\")\n",
    "yaml_prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "yaml_prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f65aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Composition: Combine prompts\n",
    "from langchain\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role-playing assistant.\n",
    "    And you are impersonating a {character}.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "\n",
    "    {example}\n",
    "\n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "full_prompt.format(\n",
    "    character=\"Pirate\",\n",
    "    example_question=\"What is your location?\",\n",
    "    example_answer=\"Arrg!! This is a secret!! ARrrg\",\n",
    "    question=\"What is your fav food?\"\n",
    ")\n",
    "\n",
    "chain = full_prompt | chat\n",
    "chain.invoke({\n",
    "    \"character\": \"Pirate\",\n",
    "    \"example_question\": \"What is your location?\",\n",
    "    \"example_answer\": \"Arrg!! This is a secret!! ARrrg\",\n",
    "    \"question\": \"What is your fav food?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e417ab",
   "metadata": {},
   "source": [
    "# 4.5~4.6 for save money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Caching: Save and reuse all responses of LM\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "chat.predict(\"How do you make italian lasagna\")\n",
    "\n",
    "# database에 caching\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 set_debug: show log\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96fcf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 요금/call\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju?\")\n",
    "    b = chat.predict(\"What is the recipe for sourdough?\")\n",
    "    print(a,b,\"\\n\")\n",
    "    print(usage)\n",
    "    print(usage.total_cost)\n",
    "    print(usage.prompt_tokens)\n",
    "    print(usage.completion_tokens) # model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f4d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Serialization: llm settings 저장\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "saved_llm = OpenAI(\n",
    "    temperature=0.1,\n",
    "    max_tokens=450,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "saved_llm.save(\"model.json\")\n",
    "\n",
    "loaded_llm = load_llm(\"model.json\")\n",
    "loaded_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b21085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "text_memory = ConversationBufferMemory()\n",
    "chat_memory = ConversationBufferMemory(return_messages=True) # chat model이 사용할 수 있는 chat message 형태로 return\n",
    "\n",
    "text_memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "text_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=2, # How many messages to save\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "add_message(1,1)\n",
    "add_message(2,2)\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "add_message(3,3)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7daa682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 ConversationSummaryMemory\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "add_message(\"South Kddorea is so pretty\", \"I wish I could go!!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a0b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150, # 메시지 요약 전 최대 토큰 수,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "add_message(\"How far is Korea from Argentina?\", \"I don't know! Super far!\")\n",
    "add_message(\"How far is Brazil from Argentina?\", \"I don't know! Super far!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68888336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"input\": \"Who is Nicolas\"})\n",
    "\n",
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"input\": \"What does Nicolas like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Memory on LLMChain(off-the-shelf;general-purpose)\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful AI talking to Human.\n",
    "\n",
    "{chat_history}\n",
    "Human: {question}\n",
    "You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True # for debugging\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Bach\")\n",
    "chain.predict(question=\"I live in Seoul\")\n",
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c73d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.6 Chat Based Memory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # unlimited messages\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Bach\")\n",
    "chain.predict(question=\"I live in Seoul\")\n",
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ad931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.1 Data Loaders and Splitters\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "txt_loader = TextLoader(\"./files/document.txt\")\n",
    "pdf_loader = PyPDFLoader(\"./files/document.pdf\")\n",
    "loader = UnstructuredFileLoader(\"./files/document.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 문장이나 문단 단위로 나눈다.\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500, # max_characters\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "# 특정 문자열을 기준으로 나눈다.\n",
    "line_break_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# chunk_size를 len(글자수)로 세지 않고, 모델과 같은 방식으로 token 갯수로 센다.\n",
    "token_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# 1번째 방법\n",
    "splitter.split_documents(docs)\n",
    "# 2번째 방법\n",
    "len(loader.load_and_split(text_splitter=splitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23afd6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.4 Embedding Models\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings\n",
    "embedder.embed_query(\"Hi\")\n",
    "\n",
    "vector = embedder.embed_documents([\n",
    "    \"how\",\n",
    "    \"are\",\n",
    "    \"you longer\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f13a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.4 Vector Store\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/document.docx\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "vectorstore.similarity_search(\"Where does Winston live?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbabe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6 RetrievalQA: off-the-shelf chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/document.docx\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(), # retrieve from\n",
    ")\n",
    "chain.run(\"Describe the Victory Mansions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e409204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.8 Stuff LCEL Chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/document.docx\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer question using only following context. If you don't know the answer, just say you don't know. don't make it up: \\n\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25368c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.9 Map Reduce LCEL Chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# 3\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "     Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatium.\n",
    "     -----\n",
    "     {context}\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs['documents'],\n",
    "    question = inputs['question']\n",
    "\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": question,\n",
    "    }).content\n",
    "    for doc in documents)\n",
    "\n",
    "    ''' #3\n",
    "    results = []\n",
    "    \n",
    "    for document in documents:\n",
    "        result = map_doc_chain.invoke({\n",
    "            \"context\": document.page_content,\n",
    "            \"question\": question,\n",
    "        }).content\n",
    "        result.append(result)\n",
    "\n",
    "    # 4\n",
    "    results = \"\\n\\n\".join(results)\n",
    "\n",
    "    return results\n",
    "    '''\n",
    "\n",
    "                        # 2\n",
    "map_chain = {\"documents\": retriever, \"question\": RunnablePassthrough()} | RunnableLambda(map_docs) | llm\n",
    "\n",
    "# 5\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"\"\"\n",
    "     Given the following extracted parts of an long document and a question, create a final answer.\n",
    "     If you don't know the answer, just say you don't know. Don't try to make up an answer.\n",
    "     ------\n",
    "     {context}\n",
    "     \"\"\"),\n",
    "     (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "final_chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "# 1\n",
    "final_chain.invoke(\"Where does Winston go to work?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
