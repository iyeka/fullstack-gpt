{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac96b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 LLMs and Chat Models\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "llm.predict(\"How many planets are there?\")\n",
    "chat.predict(\"How many planets are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99087e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Predict Messages with Chat Model\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are Madalena Bay. Reply as if you were her.\"),\n",
    "    AIMessage(content=\"It's me, Imaginal Disk\"),\n",
    "    HumanMessage(content=\"What songs you are influenced by?\")\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 PromptTemplate without from_template\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "template.format(country=\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446a713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Customize prompt with Prompt Template\n",
    "\n",
    "from langchain.chat_models import  ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"What songs did Magdalena Bay listen when making {track}?\"\n",
    ")\n",
    "prompt = template.format(track=\"Imaginal Disk\")\n",
    "\n",
    "chat.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Customize prompt with Chat Prompt Template\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "    (\"ai\", \"Ciao, mi chiamo {name}!\"),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"What is the distance between {country_a} and {country_b}. Also, what is your name?\",\n",
    "    ),\n",
    "])\n",
    "prompt = template.format_messages(\n",
    "    language=\"Greek\",\n",
    "    name=\"Socrates\",\n",
    "    country_a=\"Mexico\",\n",
    "    country_b=\"Thailand\",\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae118e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Concept of OutputParser\n",
    "\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(',')\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "p.parse(\"It's ,you, the,  purest,you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Apply OutputParser\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(',')\n",
    "        return list(map(str.strip, items))\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "    \"\"\"\n",
    "     You are a list generating machine. \n",
    "     Answer question with these conditions.\n",
    "     - COMMA SEPERATED LIST\n",
    "     - List have maximum {max_items} items.\n",
    "     - All characters in lowercase\n",
    "    \"\"\"),\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "prompt = template.format_messages(\n",
    "    max_items=5,\n",
    "    question=\"What are the pokemons?\"\n",
    ")\n",
    "\n",
    "result = chat.predict_messages(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 OutputParser and LCEL\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "            \"system\",\n",
    "            \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase.Do NOT reply with anything else.\",\n",
    "    ),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(',')\n",
    "        return list(map(str.strip, items))\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\"max_items\":5, \"question\":\"What are the pokemons?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091261e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Chaining Chains\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # listening to events on LLM\n",
    ")\n",
    "original_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a pro baker. You create a easy to follow recipe that anyone can succeed with.\"),\n",
    "    (\"human\", \"I want to bake {bread_type}.\")\n",
    "])\n",
    "original_chain = original_prompt | chat\n",
    "\n",
    "modify_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are a Korean baker. You change the original baking recipe to Koreans love. You add and adjust some ingredients. If you don't know how to convert, just say so.\"),\n",
    "    (\"human\", \"{recipe}\")\n",
    "])\n",
    "modify_chain = modify_prompt | chat\n",
    "\n",
    "final_chain = {\"recipe\": original_chain} | modify_chain\n",
    "# modify_chain.invoke({\"recipe\": original_chain_result})\n",
    "\n",
    "final_chain.invoke({\"bread_type\": \"Country Bread\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 FewShotPromptTemplate: Give examples to the model how to answer.\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# -- 1. Get examples from database\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# -- 2. Create prompt to format examples\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "# example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "# -- 3. Give examples to the FewShotPromptTemplate\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\", # same format as the question of examples\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"country\":\"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 FewShotChatMessagePromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "    (\"ai\", \"{answer}\"),\n",
    "])\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert. You give short answers.\"),\n",
    "    example_prompt,\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "chain.invoke({\"country\":\"Turkey\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 LengthBasedExampleSelector: 비용이나 모델 제약 때문에 모델에 보낼 examples를 select 해야할 때\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=80,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Making your own example selector: Random Example Selector\n",
    "'''\n",
    "ex)\n",
    "- if user is logged-in or not\n",
    "- language of the user \n",
    "'''\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples: list[dict[str, str]]):\n",
    "        self.examples = examples\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "    \n",
    "    def add_example(self, example): # add_example to already existing examples list\n",
    "        self.examples.append(example)\n",
    "    \n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "example_prompt = PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "example_selector = RandomExampleSelector(examples=examples)\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "prompt.format(country=\"Brazil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc105e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of Germany'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.4 Serialization: Load PromptTemplates from disk\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "json_prompt = load_prompt(\"./prompt.json\")\n",
    "yaml_prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "json_prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425fcd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Composition: 여러 prompts 합치기\n",
    "\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "\n",
    "    {example}\n",
    "\n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "chain = full_prompt | chat\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a4394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Caching: Save responses from LM\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache # memory에 caching하기 때문에 notebook을 재시작하면 refreshed.\n",
    "from langchain.cache import SQLiteCache # Save in database\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "chat.predict(\"How to make Italian Pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 SetDebug: 무슨 일을 하고 있는지 로그 기록을 보여준다.\n",
    "\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "set_debug=True\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "chat.predict(\"How to make Italian Pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef377f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 지출 비용 알리미\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju?\")\n",
    "    b = chat.predict(\"What is the recipe for sourdough?\")\n",
    "    print(usage)\n",
    "    print(usage.total_cost)\n",
    "    print(usage.total_tokens)\n",
    "    print(usage.prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b214abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Serialization: Save models\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "chat = OpenAI(\n",
    "    temperature=0.1,\n",
    "    model=\"gpt-5-nano\",\n",
    "    max_tokens=450,\n",
    ")\n",
    "\n",
    "chat.save(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Serialization: Load models\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "chat = load_llm(\"model.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
